\documentclass[12pt]{article}
\usepackage{graphicx, verbatim, ulem, fancyvrb, setspace,xspace,colortbl}
\usepackage{listings} 
\usepackage{color}

\setlength{\topmargin}{-.8 in}
\setlength{\textheight}{9.5  in}
\setlength{\oddsidemargin}{-.2in}
\setlength{\evensidemargin}{-.2in}
\setlength{\textwidth}{6.9in}

\begin{document}
\subsection{DBScan}

The DBScan algorithm was originally introduced by a group of researchers at the University of Munich to address several weaknesses of traditional clustering algorithms.  In particular, DBScan:

\begin{enumerate}
\item Does not require domain-specific knowledge
\item Can discover clusters of arbitrary shape
\item Is efficient on large data sets
\end{enumerate}

There are several variants of the DBScan algorithm, but all require two parameters: $Eps$, a radius value to determine whether points are in a given neighborhood, and $MinPts$, the minimum number of points necessary for a point to be considered a core point.

The first step is to categorize the points as being core, boundary, or noise points.  A core point is one where for a point $q$ and neighborhood function $N_{Eps}$, $|N_{Eps}(q)|\geq MinPts$, a boundary point is one where $MinPts>|N_{Eps}(q)|\geq 1$, and a noise point is a point where $|N_{Eps}(q)|= MinPts$

Next, points are assigned to an arbitrary number of clusters.  The algorithm iterates through each point $p \in Core$ and assigns an updated cluster label to $p$ if a cluster label has not been assigned, and then assigns the same label to every $p'$, where $p' \in N_{eps}(p)$.

That the number of clusters found by DBScan is unspecified is a major advantage of the algorithm, but it also means that values for $Eps$ and $MinPts$ should be chosen carefully.  Similarly, while DBScan is computationally efficient in that it doesn't require convergence or multiple iterations, the efficiency of the implementation of the neighborhood function can make a big difference in terms of performance.  An additional consideration is that DBScan will produce different clusters depending on the order of the data, which may be an important consideration in some applications.

\subsection{Competitive Learning}
Competitive learning refers to a class of neural networks in which nodes "compete" to best fit a given set of inputs.  Competitive learning was first introduced in 1985 by D. Rumelhart and D. Zipser.  

The competitive learning network implemented here uses a topology with one input layer, one output layer and no hidden layers.  Output layer weights were initialized using $U(0,1)$ draws, with the number of output nodes set to the number of unique classes in the original dataset, and all input vectors were normalized.  

The training process involved randomly drawing input vectors from the data set and comparing these to the weights of each output node using a similarity function calculated as the inverse Euclidean distance between the two vectors.  A bias consisting of the proportion of times that particular node had already "won" was added to minimize the problem of dead neurons, in which some output nodes are initialized with values such that they never "win."  The output node with the lowest adjusted similarity score was updated by adding the values of the input vector (scales by the tunable parameter $\eta$) to the weights of that node and then normalizing its weights.  Convergence was considered to have occurred when the maximum difference between the node biases was less than a tolerance of $0.0005$ for more than ten input vectors.

\subsection{tuningparameters}
DBScan:
MinPts was determined by the formula $max(\lceil n_{pts}/n_{clust} \rceil ,2)$, where $n_{pts}$ is the total number of input vectors and $n_{clust}$ is the number of clusters expected.  Eps was determined heuristically based on MinPts and other attributes of the particular data set. 

Competitive Learning:
$\eta$, the scaling factor for updates, was set to 0.25.  The number of output nodes in a given network was determined by the number of unique classes in the original data set.

\end{document}
 
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: