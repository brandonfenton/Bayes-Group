\documentclass[11pt,titlepage]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% packages
\usepackage{float} % for better behaved floats
\usepackage{graphicx} % for pictures
%\usepackage{fancyhdr} % for headers
\usepackage{verbatim} % displaying r code
%\usepackage{fancyvrb}
\usepackage{setspace} % vspace and hspace
%\usepackage{listings}
\usepackage{enumitem} % for [label=]
\usepackage{amsmath}  % math symbols and such
\usepackage{amssymb}
%\usepackage{lastpage} % for page numerbering
\usepackage{color}
\usepackage{multirow}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% margins
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}

\setlength{\parskip}{5.5pt} % Skip half a line before and after each par
\setlength{\parindent}{0pt} % No auto-indent
% \def\fs{\footnotesize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%put%words%in%circle%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage{tikz}
% \usetikzlibrary{shapes.misc,shadows}
% \usetikzlibrary{arrows}
% \usetikzlibrary{shapes}
% \newcommand{\mymk}[1]{%
%   \tikz[baseline=(char.base)]\node[anchor=south west,
%     draw = blue, rectangle, rounded corners, inner sep=2pt,
%     minimum size=7mm, text height=2mm](char){\ensuremath{#1}} ;}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% title page
\title{Team BadAss}
\author{K. Flagg, B. Fenton, \& D. Anderson}
\date{}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle
\tableofcontents
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% R code options
<<setup, include=FALSE, cache=FALSE>>=
### set default values for code chunks
opts_chunk$set(
  echo=FALSE,
  message=F,
  warning=F,
  fig.width=10,
  fig.height=3,
  out.width='\\linewidth',
  out.height='0.3\\linewidth',
  dev='pdf',
  concordance=TRUE,
  results='asis',
  size = 'footnotesize',
  fig.align='center'
)

### other defaults
options(
  replace.assign=TRUE,
  width=72,
  digits = 3,
  max.print="72",
  show.signif.stars = FALSE
)

### load in necessary packages
# require(nlme)
require(lme4)
require(xtable)
# require(ggplot2)
# require(lattice)
# require(rjags)
# require(R2jags)
# require(mcmcplots)
# require(LearnBayes)
# library(mvtnorm)
# library(coda)
# require(geoR) # for scaled inverse chi squared
require(dplyr)
require(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
### set working directory - Doug
# setwd("C:/Users/dsand_000/Desktop/Stats/S532/gitProj/Bayes-Group/report")
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
Bayesian inference is becoming a popular topic as of late; such is the case for fields like Ecology. However, Bayesian data analysis is rarely utilized in the Consulting Seminar because few clients are familiar with the methodology. We hope to illustrate that, while Bayesian data analysis may be less familiar than its likelihood or frequentist counterparts, it can provide results when other approaches do not. Additionally, it is our goal to provide easy-to-understand implementations of Bayesian inference. This report will implement both a likelihoodist and Bayesian approach in order to compare results.

\section{Objectives}
Researchers are interested in the effect of the timing of a nitrogen treatment on the rate of Wheat Streak Mosaic Virus infection for several varieties of winter wheat. It is also of interest to see whether the strength of the effect differs between different varieties, or between environments where the virus is present in high levels versus low level. In this report, we will demonstrate the Analysis of Variance to determine which variables and interaction are the most important in explaining variation in infection rate.

\section{Design}
The experiment was performed in six blocks. Each block contained five rows, with one of five varieties of wheat randomly assigned to each row. Each row was split into six plots, and each plot was assigned a unique combination of nitrogen application time (early, mid, and late) and inoculation status (inoculated or not). Infected plants were transplanted into the inoculated plots so that the inoculated plots were known to have an elevated virus presence. Non-inoculated plots were assumed to have the virus present in a naturally-occurring level.

In total, 180 plots were observed. From each plot, 30 leaves were collected haphazardly and the infection status was recorded for each. These data were aggregated to the plot level so that the response variable was the number of infected plants out of 30.

\section{Model and Simulation}
<<data_simulation>>=
## These functions will be used thoughout.
logit <- function(x){return(log(x/(1-x)))}
expit <- function(x){return(exp(x)/(1+exp(x)))}

## Here, we set up a data frame to store our data: factors of each
##   variable for each plot.
design <- data.frame('block' = as.numeric(gl(6, 30)),
                     'row' = as.numeric(gl(30, 6)),
                     'plot' = 1:180,
                     'variety' = numeric(180),
                     'inoc' = numeric(180),
                     'nitrogen' = numeric(180))
set.seed(6823)
## Each column is a permutation of variety indices
permute.v <- matrix(replicate(6, sample(5, 5, replace = FALSE)), ncol = 6)
for(i in 1:180){
  design$variety[i] <- permute.v[(design$row[i]-1)%%5+1, design$block[i]]
}
ni <- matrix(c(1, 1, 1, 2, 2, 2, 1:3, 1:3), ncol = 2)
## Each column is a permutation of inoc:nitrogen indices
permute.ni <- matrix(replicate(30, sample(6, 6, replace = FALSE)), ncol = 30)
for(i in 1:30){
  design[((i-1)*6+1):(i*6), c('inoc', 'nitrogen')] <- ni[permute.ni[,i],]
}



#### Start the process of simulating data.
## For the binomial setting we have
##    y ~ Bin(30,pi) where
##    logit(pi) = Xb and
##    X is the model matrix and b is a vector of coefficients

# Determine a value of mu to be used for the intercept (first row of b vector).
#   We found that logit(0.05) is approximately -3.
mu <- -3

# It is good practice to set seed when simulating data as it will be
#   reproducible.
set.seed(93)

## We can set values for the main coefficients or use random draws.
##   These are also rows of the b vector.
# There are six blocks.
b.eff <- rnorm(6,0,0.01)
# There are five varieties.
v.eff <- rnorm(5,0,0.3)
# There are three nitrogen levels.
n.eff <- sort(rnorm(3,0,0.4))
# There are two inoculation levels.
i.eff <- c(-0.5,0.5)

## We can do the same for all the two-way interactions.
##   These are more rows of the b vector.
# There are 30 combinations of block and variety.
bv.eff <- rnorm(30,0,0.00001)
# There are 18 combinations of block and nitrogen.
bn.eff <- rnorm(18,0,0.00001)
# There are 12 combinations of block and inoculation.
bi.eff <- rnorm(12,0,0.00001)
# There are 15 combinations of variety and nitrogen.
vn.eff <- rnorm(15,0,0.01)
# There are 10 combinations of variety and inoculation.
vi.eff <- rnorm(10,0,0.03)
# There are 6 combinations of nitrogen and inoculation.
ni.eff <- rnorm(6,0,0.03)

## We can also establish effects for three-way interactions.
##   These are also in the b vector.
# There are 90 combinations of block, variety, and nitrogen.
bvn.eff <- rnorm(90,0,0.00001)
# There are 60 combinations of block, variety, and inoculation.
bvi.eff <- rnorm(60,0,0.00001)
# There are 36 combinations of block, nitrogen, and inoculation.
bni.eff <- rnorm(36,0,0.00001)
# There are 30 combinations of variety, nitrogen, and inoculation.
vni.eff <- rnorm(30,0,0.001)

## It was not of interest to include a four-way interaction.



#### We can now update our "design" matrix with the levels of each
####   interaction.
# head(design) # reminder of the layout of design
## Find the levels for each two-way interaction assigned to a plot.
bv.int <- as.numeric(interaction(design[,1],design[,4]))
bn.int <- as.numeric(interaction(design[,1],design[,6]))
bi.int <- as.numeric(interaction(design[,1],design[,5]))
vn.int <- as.numeric(interaction(design[,4],design[,6]))
vi.int <- as.numeric(interaction(design[,4],design[,5]))
ni.int <- as.numeric(interaction(design[,6],design[,5]))
## Find the levels for each three-way interaction assigned to a plot.
bvn.int <- as.numeric(interaction(design[,1],design[,4],design[,6]))
bvi.int <- as.numeric(interaction(design[,4],design[,6]))
bni.int <- as.numeric(interaction(design[,4],design[,5]))
vni.int <- as.numeric(interaction(design[,6],design[,5]))

## Once the levels have been established for each interaction (of
##   each plot) we can include them as columns in our data set.
##   We have opted to call the data set 'od' for original data.
od <- data.frame(
  "blk" = factor(design[,1]),
  "vty" = factor(design[,4]),
  "nit" = factor(design[,6]),
  "ino" = factor(design[,5]),
  "bv.int" = factor(bv.int),
  "bn.int" = factor(bn.int),
  "bi.int" = factor(bi.int),
  "vn.int" = factor(vn.int),
  "vi.int" = factor(vi.int),
  "ni.int" = factor(ni.int),
  "bvn.int" = factor(bvn.int),
  "bvi.int" = factor(bvi.int),
  "bni.int" = factor(bni.int),
  "vni.int" = factor(vni.int)
)

## Using the levels of each factor for a plot, we can find the probabilities
##   associated with the simulated data.
# We can start on the logit scale: logit(pi) = Xb.
logit.pi.vec <- mu + b.eff[od[,1]] + v.eff[od[,2]] + n.eff[od[,3]] +
  i.eff[od[,4]] + bv.eff[od[,5]] + bn.eff[od[,6]] + bi.eff[od[,7]] +
  vn.eff[od[,8]] + vi.eff[od[,9]] + ni.eff[od[,10]] + bvn.eff[od[,11]] +
  bvi.eff[od[,12]] + bni.eff[od[,13]] + vni.eff[od[,14]]

# Now, transform those back to the pi (probability) scale.
pi.vec <- apply(cbind(logit.pi.vec), 1, expit) # pi = expit(X * beta)

# We can use the probabilities to generate y's: y_i ~ Bin(30,pi_i). These will
#   be the counts of infected leaves for a given plot.
set.seed(324)
y.vec <- apply(cbind(pi.vec), 1, function(p) rbinom(1,30,p))

## Lastly, we can attach the counts of infected leaves and uninfected leaves
##   for each plot to our data set.
od$"infected" <- y.vec
od$"uninfected" <- 30 - y.vec

## If desired, it is possible to write the data set to a csv or table for
##   future use.
# write.csv(od,"od.csv",row.names = FALSE)
@

The statistical model is
\begin{align*}
y_j &\sim \text{Bin}(30,\pi_j), \ \text{for} \ j=1,2,\dots,180; \\
\text{logit}(\pi_j)
% OVERALL
&= \mu % Batch 0: mean
% BLOCK LEVEL
+ \beta^{(1)}_{b[j]} % Batch 1: Blk
% ROW LEVEL
+ \beta^{(2)}_{v[j]} % Batch 2: Vty
+ \beta^{(3)}_{b[j],v[j]} % Batch 3: Blk * Vty
% PLOT LEVEL
+ \beta^{(4)}_{i[j]} % Batch 4: Ino
+ \beta^{(5)}_{n[j]} % Batch 5: Nit
\\
&\quad
+ \beta^{(6)}_{b[j],i[j]} % Batch 6: Blk * Ino
+ \beta^{(7)}_{b[j],n[j]} % Batch 7: Blk * Nit
+ \beta^{(8)}_{i[j],n[j]} % Batch 8: Ino * Nit
+ \beta^{(9)}_{i[j],v[j]} % Batch 9: Ino * Vty
+ \beta^{(10)}_{n[j],v[j]} % Batch 10: Nit * Vty
\\
&\quad
+ \beta^{(11)}_{b[j],i[j],n[j]} % Batch 11: Blk * Ino * Nit
+ \beta^{(12)}_{b[j],i[j],v[j]} % Batch 12: Blk * Ino * Vty
+ \beta^{(13)}_{b[j],n[j],v[j]} % Batch 13: Blk * Nit * Vty
+ \beta^{(14)}_{i[j],n[j],v[j]}; % Batch 14: Ino * Nit * Vty
% Maybe add the four-way?
\\
\beta^{(k)}_{(\cdot)}&\sim\mathrm{N}(0,\sigma^2_k),
\ \text{for} \ k=1,2,\dots,14;
\end{align*}
where \(b=1,2,\dots,6\) indexes the blocks; \(i=1\) indicates non-inoculated status and \(i=2\) indicates inoculated status; \(n=1,2,3\) indicate early, mid, and late nitrogen application, respectively; \(v=1,2,\dots,5\) indexes the varieties.

In the above model, each \(\beta^{(k)}_{(\cdot)}\) is a member of a batch of coefficients sharing a common Normal distribution with a scale parameter \(\sigma_k\). The analysis of variance is less concerned with the values of particular coefficients than with the contribution of each batch to the total variation of the data, so the blocks, varieties, and nitrogen application times can be thought of as random samples from populations.

To understand the model, it is helpful to think about the process that would generate the data, and then simulate data from that process. There is some overall infection rate for the population of all winter wheat plants. Plants in inoculated plots will have higher infection rates than plants in plots that were not inoculated. Characteristics like variety, timing of the nitrogen application, and environmental conditions (for which block is a proxy) cause variation about the overall mean. If a certain characteristic, such as the variety, has a large influence on the infection rate, then the coefficients in batches associated with variety will have a large amount of variation and a high standard deviation.

Since infection is an uncommon event, an overall infection rate of 5\% could be expected. For the simulation, we set \(\mu=-3\) because \(\mathrm{logit}(0.05)\approx -3\). Inoculated plots will have higher infection rates than non-inoculated plots, so \(\beta^{(4)}_2=0.5\) and \(\beta^{(4)}_1=-0.5\) were used to give baseline infection rates of about 8\% and 3\% in inoculated and non-inoculated plots, respectively.

For each batch of coefficients, values were generated via random draws from normal distributions
centered at zero. Variances were chosen for each effect to reflect our beliefs about the importance
of that effect on the response.  We have provided code so that all details and steps of the process
are available and reproducible.  The simulated leaf counts can be seen in Figure~\ref{simplot}.

\begin{figure}[H]
<<simulation_plot,out.width="0.4\\linewidth",fig.width=4>>=
par(mfrow=c(1,1), mar=c(2,2,2,1))
plot(table(y.vec), ylab = "", xlab = "")
  points(sort(unique(y.vec)), table(y.vec), pch=19, col="blue")
@
\caption{The number of the 180 simulated plots that had 0 leaves, 1 leaf, $\hdots$, 11 leaves infected out of the 30 collected from each plot.}
\label{simplot}
\end{figure}

\begin{figure}[H]
<<heat_map,out.height='0.3\\linewidth',fig.height=3>>=
## Heatmap with meaningful ordering

# Order the dataset, using the arrange function from the dplyr package
od.ordered <- dplyr::arrange(od, vty, blk, ino, nit)

# Create a matrix of the arranged responses
infected.arranged <- matrix(od.ordered$infected, ncol = 6, byrow = TRUE)

# Set up two panels, right one for a legend
layout(t(1:2), widths = c(9, 1))

# Plot the heatmap, with zeros in black and segments separating the blocks
par(mar = c(3, 10, 6, 2)) # Set big margins
image(z = infected.arranged, y = 1:6,

      # Variety blocks are 1 unit wide, centered at 0.5, 1.5, etc
      x = seq(0.5, 5.5, 1/6),

      # Use black for 0, and use heatmap colors (red-orange-yellow-white)
      # for 1 to 30
      col = c("black", heat.colors(30)), zlim = c(0, 30),

      # Don't automatically create axes or labels
      xlab = "", ylab = "", yaxt = "n", xaxt = "n")

# Use white line segments to visually separate the varieties
segments(x0 = 1.5:4.5, y0 = 0.5, y1 = 6.5, col = "white")

# Place a title at the top, and label Varieties, treatment:status levels,
# and blocks around the image
title("Infection Counts", line = 4)
mtext("Block", 3, 2)
axis(3, labels = rep(levels(od.ordered$blk), 5), cex.axis = 0.75,

     # Each block is plotted in a column with width 1/6, so put the
     # labels in the middle
     at = seq(7/12, 5 + 5/12, 1/6))
mtext("Nitrogen.Inoculation", 2, 3)
axis(2, labels = levels(with(od.ordered, interaction(nit, ino))),
     at = 1:6, las = 2)
mtext("Variety", 1, 2)
axis(1, labels = levels(od.ordered$vty), at = 1:5)

# Legend
par(mar = c(3, 1, 6, 2))
image(y = seq(-0.5, 30.5, 1), z = matrix(0:30, nrow = 1), axes = FALSE,
      ylab = "",col = c("#000000", heat.colors(30)), zlim = c(0, 30))
title("Legend", line = 1.5)
axis(4)
@
\caption{Heatmap of the number of infected leaves out of 30 from each plot. The data have been arranged by variety, block, inoculation status, and nitrogen application time so that patterns can be seen easily.}
\label{heatmap}
\end{figure}

\newpage

\section{Generalized Linear (Mixed) Model}
\subsection{Likelihood}
The likelihood function provides a relative measure of how well a possible
value of the parameter \(\theta\) fits with the observed data \(y\), and
so it can be used to identify the value of \(\theta\) that is then used
as the estimate most consistent with \(y\). The likelihood function takes
its form from the probability model for \(y\), denoted \(p(y|\theta)\).
However, we discuss likelihoods after the data have been collected so we
consider \(y\) to be fixed and we examine how \(p(y|\theta)\) varies as a
function of \(\theta\).

Since \(p(y|\theta)\) is the probability of observing \(y\) for a given
\(\theta\), a \(\theta\) where \(p(y|\theta)\) is large is more
consistent with the obeserved data than a \(\theta\) value that makes
\(p(y|\theta)\) small. Then a reasonable approach to estimating
\(\theta\) is to find the value of \(\theta\) that maximizes
\(p(y|\theta)\) for the observed \(y\).


Note that the likelihood function depends on the particular data
that were observed. The data \(y'\) could very well have been collected
instead of \(y\), and then the likelihood function would be
\(p(y'|\theta)\), which might be maximized by a different \(\theta\) value
than the \(\theta\) value the maximizes \(p(y|\theta)\). If we consider
how this estimate of \(\theta\) will vary between different samples,
we can construct a probability distribution.


\subsection{Likelihood Principle}
The idea behind the likelihoodist approach to making inference stems from the Likelihood Principle. A likelihoodist believes that given a statistical (probability) model, all of the information in a sample relevant to model parameters is contained in the likelihood function. The benefit to using the maximum likelihood estimate (MLE) is that, assuming the model is correct, the sample size is large enough and the sampling method is unbiased, then  the MLEs are unbiased, the standard deviations of the sampling distributions of the estimators are easy to formulate, the estimators have relatively high precision, and the shapes of the sampling distribution are approximately normal (The Statistical Sleuth, page 611).

\subsection{Likelihood to GLMER}
The likelihoodist approach to making inferences about split-plot experiments requires the use of linear mixed models.
In this case we are analyzing simulated count data, so a generalized linear mixed model is appropriate.
The \texttt{glmer()} function from the \texttt{lme4} package in the base R distribution was used to fit this model.

As nitrogen and inoculation were pre-assigned to plots, they are treated as constants (in other words, fixed effects).  Block and variety levels can be considered as members of a larger population, and as a result will be treated as a varying, or random, effect.
<<glmm, eval=FALSE>>=
## The lme4 package has the glmer function.
require(lme4)

## Fit the model using glm.
glm.mod <- glmer(formula = cbind(infected, uninfected) ~ (nit + ino)^2 +
          (1 | blk/vty) + (1|blk:nit) + (1|blk:ino) + (1|vty:nit) + (1|vty:ino) +
          (1 | vty:nit:ino) + (1 | blk:nit:ino) + (1 | blk:vty:nit) + (1 | blk:vty:ino),
          data = od, family = binomial(link = "logit"),
      control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2000)))
## Produce summary output for the model.
# summary(glm.mod)
# anova(glm.mod, test="Chisq")
@

\subsection{Results}
-caterpillar plot?
-
\subsection{Diagnostics}
-binned residual plot
\subsection{Arguments against Likelihood}
While the likelihoodist approach can be powerful and elegant in its simplicity, it has some major shortcomings.  Even if the model is correct, two different models can have the same likelihood function (as is the case for binomial and negative binomial distributions).  Similarly, even if the sample is sufficiently large and was collected in such a way that it should be representative, it is still possible to end up with a sample that is not representative of the population.  Either of these flaws could be tempered by including information about prior beliefs or domain-specific knowledge relevant to the model, but the likelihoodist approach does not provide a way to do so.  In some cases this may not matter, but in many others the relevant information which is left unused by the likelihoodist approach can greatly improve both the ability to make inferences and the quality of those inferences.

\section{Bayesian Analysis}
\subsection{Background}
Probability theory provides the basic property of conditional probability known as Bayes' rule,
\begin{align*}
p(\theta|y) & = \frac{p(\theta,y)}{p(y)} \\
& = \frac{p(\theta)p(y|\theta)}{p(y)}
\end{align*}
where $p(y) = \sum_{\theta} p(\theta)p(y|\theta)$ is summed over all possible values of $\theta$. A Bayesian analysis starts from Bayes' rule. It involves finding a posterior distribution for
\(\theta\), denoted \(p(\theta|y)\), from the likelihood \(p(y|\theta)\)
and a prior distribution of \(\theta\), \(p(\theta)\). The posterior is
found using the proportionality relationship,
\begin{align*}
p(\theta|y)\propto p(\theta)\times p(y|\theta)\text{.}
\end{align*}
Often, analysts assume that all values of \(\theta\) are approximately
equally probable, and so the prior distribution is constant:
\(p(\theta)=c\). Then the posterior distribution is proportional to the
likelihood,
\begin{align*}
p(\theta|y)\propto c\times p(y|\theta)\propto p(y|\theta)\text{.}
\end{align*}
In cases like this, the value of \(\theta\) that is most probable in
posterior is the same as the maximum likelihood estimate, so it is common
for Bayesian inference to appear similar to likelihood-based inference.
It is important to remember, however, that a likelihood is not a probabilty
statement about $\theta$. A Bayesian posterior distribution is used to make
probability statements about \(\theta\), and a likelihood-based
\emph{estimator} has a probability distribution, but the likelihood itself
can only be used to compare possible \(\theta\) values in the context of
one set of observed data.

\subsection{Why Choose Bayesian}
There are many advantages to performing a Bayesian analysis. A big one in this case, is its ability to provide meaningful results. MLE at edge of parameter space.

The likelihoodist approach provides a point estimate; a Bayesian approach provides a probability distribution.

It is very difficult to fit a generalized linear mixed model in R, and can have separation issues. We can avoid many of those complications with the Bayesian approach.

It can also come in handy when not much data is provided. {\it This might not be necessary to include.}

\subsection{Priors}
As described in Section 6.1, the use of Bayesian analysis involves prior distributions. In practice, not much is known as to what distribution the parameters, themselves, actually follow. But, hey, we don't actually need to know! We ultimately want the data to make the most impact on the posterior distribution we obtain, so our decisions on what priors we use are really just to get the ball rolling, so to speak.

A natural choice for many who implement a Bayesian analysis is to use what is called a noninformative prior. It is noninformative in the sense that is has the largest possible variance. This variance allows for the parameter to be from anywhere the distribution is defined. Another choice is to use a minimally informative prior. There are typically some natural limits as to what that parameter could be.

There are some benefits to the choices of priors, such as conjugacy (but that doesn't come into play here as much).

\subsection{The Bayesian Model}
The model we have gone with is yada, yada, yada.

{\it Discuss the priors we chose here or create a subsection?}

The Normal probability distributions for each effect is a natural choice. Each centered at 0 as they will be adjustments on the overall mean. The half cauchy is a solid choice as argued by Gelman et al in a ``Weakly Informative Paper''. The spread of 3 allows for the effects to differ by an effect size of 18, which on the logit scale is enormous, as argued by some page in BDA3.

\subsection{McSTANislaw McUlam}
When conjugate priors are used we have the benefit of knowing the  distribution of the posterior. However, given the large capacity of parameters in the model we have fit and the prior distributions chosen, the posterior is not as straightforward. This would result in very intensive, and possibly lack of computing power, calculations to determine $p(y)$ -- the denominator in Section 6.1. However, there is a very convenient way around this issue.

Markov Chain Monte Carlo (MCMC) is the process of jumping around the parameter space in order to obtain draws from some target distribution (in this case the posterior distribution). As with all fields of mathematics and statistics we put our trust in the infinite; in that, it is the belief that if we were to sample enough times we will have converged on to a distribution that is the true posterior.

R isn't the fastest of programming environments, and given the quantity of parameters we hope to estimate, it would run slowly. The way we can harness the power of MCMC will be through STAN, which adapts the code to run in C -- much faster. There are alternative out there, but this one is rather brilliant. STAN uses a type of MCMC algorithm called Hamiltonian something or another which jumps around the parameter space very efficiently, like a marble rolling up, down, and around a bowl.

\subsection{Written model to STAN models}
We can take the model written above and transform it into computing lingo for STAN.

\subsection{Other compilers?}
There is also BUGS, JAGS, and BayesGlm. There is a website to help you choose priors, maybe?

\section{Posterior Inferences}

<<stop_the_errors>>=
# Numbers of observations
n.o <- 180
m <- 30
cauchyscale <- 3
n.b <- 6
n.v <- 5
n.n <- 3
n.i <- 2
n.bv <- n.b*n.v
n.bn <- n.b*n.n
n.bi <- n.b*n.i
n.vn <- n.v*n.n
n.vi <- n.v*n.i
n.ni <- n.n*n.i
n.bvn <- n.b*n.v*n.n
n.bvi <- n.b*n.v*n.i
n.bni <- n.b*n.n*n.i
n.vni <- n.v*n.n*n.i

# Vectors of levels
b.vec <- od$blk
v.vec <- od$vty
n.vec <- od$nit
i.vec <- od$ino
bv.vec <- od$bv.int
bn.vec <- od$bn.int
bi.vec <- od$bi.int
vn.vec <- od$vn.int
vi.vec <- od$vi.int
ni.vec <- od$ni.int
bvn.vec <- od$bvn.int
bvi.vec <- od$bvi.int
bni.vec <- od$bni.int
vni.vec <- od$vni.int

# Infected
y <- od$infected

# Load stored Stan draws.
load('../sens.rdata')
@

<<stan_unrun, eval=FALSE>>=
#### not at all useless
od.stan.data <- list(num_obs=n.o,
                     m=m,
                     cauchysd=cauchyscale,
                     num_b=n.b,
                     num_v=n.v,
                     num_n=n.n,
                     num_i=n.i,
                     num_bv=n.bv,
                     num_bn=n.bn,
                     num_bi=n.bi,
                     num_vn=n.vn,
                     num_vi=n.vi,
                     num_ni=n.ni,
                     num_bvn=n.bvn,
                     num_bvi=n.bvi,
                     num_bni=n.bni,
                     num_vni=n.vni,
                     y=y,
                     blk=as.numeric(b.vec),
                     vty=as.numeric(v.vec),
                     nit=as.numeric(n.vec),
                     ino=as.numeric(i.vec),
                     bv_int=as.numeric(bv.vec),
                     bn_int=as.numeric(bn.vec),
                     bi_int=as.numeric(bi.vec),
                     vn_int=as.numeric(vn.vec),
                     vi_int=as.numeric(vi.vec),
                     ni_int=as.numeric(ni.vec),
                     bvn_int=as.numeric(bvn.vec),
                     bvi_int=as.numeric(bvi.vec),
                     bni_int=as.numeric(bni.vec),
                     vni_int=as.numeric(vni.vec))
#stan_herpes <- stan_model(file = "./your_mom.stan", model_name = "ni")
#require(rstudioapi)
#options(mc.cores = 1)
#your_sister <- sampling(stan_herpes, chains = 4, iter = 500, data = weiner_logs, sample_file="trial")
# t1 <- read_stan_csv("trial_1.csv", col_major = TRUE)
# t2 <- read_stan_csv("trial_2.csv", col_major = TRUE)
# t3 <- read_stan_csv("trial_3.csv", col_major = TRUE)
# t4 <- read_stan_csv("trial_4.csv", col_major = TRUE)
trial5000 <- read_stan_csv(c("trial5000_1.csv","trial5000_2.csv","trial5000_3.csv","trial5000_4.csv"), col_major=TRUE)
# trial <- read_stan_csv(c("trial_1.csv","trial_2.csv","trial_3.csv","trial_4.csv"), col_major=TRUE)
# plot(your_sister, pars=c("sigma_b", "sigma_v", "sigma_n", "sigma_i", "sigma_bv", "sigma_bn","sigma_bi", "sigma_vn", "sigma_vi", "sigma_ni", "sigma_bvn", "sigma_bvi", "sigma_bni", "sigma_vni"), ci_level=0.5, outer_level=0.95, point_est="median")
plot(trial5000, pars=c("sigma_b", "sigma_v", "sigma_n", "sigma_i", "sigma_bv", "sigma_bn","sigma_bi", "sigma_vn", "sigma_vi", "sigma_ni", "sigma_bvn", "sigma_bvi", "sigma_bni", "sigma_vni"), ci_level=0.5, outer_level=0.95, point_est="mean")

od.cauchy  <- stan_model(file = 'cauchy.stan', model_name = 'cauchyprior')
od.cauchy.out <- sampling(od.cauchy, chains = 4, iter = 500, data = od.stan.data, sample_file = 'trial')

# TODO move this somewhere where it will display, maybe
plot(od.cauchy.out, pars=c("sigma_b", "sigma_v", "sigma_n", "sigma_i", "sigma_bv", "sigma_bn","sigma_bi", "sigma_vn", "sigma_vi", "sigma_ni", "sigma_bvn", "sigma_bvi", "sigma_bni", "sigma_vni"), ci_level=0.5, outer_level=0.95, point_est="mean")

# A simple sensitivity analysis
scales <- c(1, 2, 3, 5, 7, 11)
sens <- lapply(scales, function(scale){
    stan.data <- list(num_obs = n.o, m = m, cauchysd = scale,
                      num_b = n.b, num_v = n.v, num_n = n.n, num_i = n.i,
                      num_bv = n.bv, num_bn = n.bn, num_bi = n.bi,
                      num_vn = n.vn, num_vi = n.vi, num_ni = n.ni,
                      num_bvn = n.bvn, num_bvi = n.bvi, num_bni = n.bni,
                      num_vni = n.vni, y = y, blk = as.numeric(b.vec),
                      vty = as.numeric(v.vec), nit = as.numeric(n.vec),
                      ino = as.numeric(i.vec), bv_int = as.numeric(bv.vec),
                      bn_int = as.numeric(bn.vec), bi_int = as.numeric(bi.vec),
                      vn_int = as.numeric(vn.vec), vi_int = as.numeric(vi.vec),
                      ni_int = as.numeric(ni.vec),
                      bvn_int = as.numeric(bvn.vec),
                      bvi_int = as.numeric(bvi.vec),
                      bni_int = as.numeric(bni.vec),
                      vni_int = as.numeric(vni.vec))
    return(sampling(od.cauchy, chains = 4, iter = 1000, data = stan.data))
  })

@

\pagebreak

\begin{figure}[H]
<<sens_analysis, fig.width=9, fig.height=12, out.width="\\textwidth", out.height="0.95\\textheight">>=
sensitivity.plot <- function(stan.list, pars, vals, xmin = 0, xmax = 1,
                             inner = 0.5, outer = 0.95,
                             ylab = 'Hyperparameter Value', ...){
# Creates a caterpillar plot comparing posterior intervals for each of
# several parameters for several values of one hyperparameter.
#  stan.list  A list of stan_fit objects
#  pars       Character vector of the parameters being examined
#  vals       Numeric vector of hyperparameter values that were used
#  xmin       Numeric (vector) of lower xlim values for each plot
#  xmax       Numeric (vector) of upper xlim values for each plot
#  inner      Confidence lever for outer interval
#  outer      Confidence level for outer interval
  if(length(xmin)<length(pars)) xmin <- rep(xmin, length(pars))
  if(length(xmax)<length(pars)) xmax <- rep(xmax, length(pars))
  for(j in 1:length(pars)){
    plot(NULL, xlim = c(xmin[j], xmax[j]), ylim = c(length(sens), 1),
         main = paste('Sensitivity Plot of', params[j]),
         xlab = params[j], ylab = ylab, yaxt = 'n', ...)
    axis(2, at = 1:length(sens), labels = scales, las = 1)
    for(i in 1:length(sens)){
      current <- unlist(extract(sens[[i]], pars = params[j]))
      segments(x0 = quantile(current, c((1-outer)/2, (1-inner)/2)),
               x1 = quantile(current, 1-c((1-outer)/2, (1-inner)/2)),
               y0 = i, lwd = c(1, 3))
      points(x = median(current), y = i, pch = 20)
    }
  }
}

params <- c("sigma_b", "sigma_v", "sigma_n", "sigma_i", "sigma_bv",
            "sigma_bn","sigma_bi", "sigma_vn", "sigma_vi", "sigma_ni",
            "sigma_bvn", "sigma_bvi", "sigma_bni", "sigma_vni")
scales <- c(1, 2, 3, 5, 7, 11)
xlims <- c(1, 1, 8, 20, 1, 1, 1, 1, 1, 3, 1, 1, 1, 5)
par(mfrow = c(5, 3), mar = c(4.1, 3.1, 3.1, 1.1))
sensitivity.plot(sens, params, scales, xmax = xlims, ylab = 'Prior Scale')
@
\caption{TODO}
\label{sens_plot}
\end{figure}

\section{Posterior Predictive Check}

<<posterior_prediction_setup>>=
# Get the one Stan fit where we used a scale of 3.
od.stan <- sens[[3]]

# Obtain posterior predictions, using probabilities from the model's posterior
#   distribution.
p.stan <- extract(od.stan, pars = 'p')$p
y.pred <- apply(p.stan, c(1, 2), function(p){return(rbinom(1, m, p))})

# Calculate leaf counts of zero infected leaves for each vector in y.pred.
n.zero.pred <- apply(y.pred, 1, function(x){return(sum(x==0))})

# Calculate max infected leaf counts for each vector in y.pred.
max.pred <- apply(y.pred, 1, max)

# Calculate absolute error between y and y.pred.
abs.err <- function(ypred) {return(mean(abs(ypred-y)))}
abs.err.pred <- apply(y.pred, 1, abs.err)

# Calculate mean and 95% posterior interval bounds for absolute error values.
m.abs.err.pred <- round(mean(abs.err.pred), 2)
abs.err.int <- round(quantile(abs.err.pred, c(0.025, 0.975)), 2)

# Find proportion of y and y.pred counts greater than three.
y.g3 <- round(mean(y > 3), 3)
yp.g3 <- round(mean(y.pred > 3), 3)
@

\subsection{Overview}
One major advantage to taking a Bayesian approach to data analysis is the inherent flexibility of Bayesian methods.  Once draws have been obtained from a posterior distribution, those draws can be used to predict new data sets which will hopefully be representative of the population from which the original data were drawn.  If this is true, then the original data should not seem extraordinary in relation to the posterior predictions.  By extension, inferences made based on the predicted values should be close to those made based on the original data.  More specifically, checking inferences about aspects of the data which were not explicitly modeled can indicate whether or not the model performs well.   Such checks, known as posterior predictive checks, should ideally compare measurements which are related to the researchers' objectives.

\subsection{Test Quantities}
There are two particular features of the original data which are important to capture in the modeling process: the relatively large number of zeros, and the maximum infected count.  A model incorporating either of these features separately would be relatively straightforward to fit, but capturing both simultaneously takes more consideration.  Since the extreme values of the data are of primary concern here, the mean is not a useful metric.  In order to ascertain whether or not the model did in fact capture these aspects of the data, posterior predictive checks for the number of predicted zeros (Figure~\ref{post_pred}, left) and predicted maximum (Figure~\ref{post_pred}, center) were performed.  The relationship between predicted counts and the original counts can be visualized using a scatterplot (Figure~\ref{post_pred}, right).

\begin{figure}[H]
<<post_pred_plots>>=
# Plot zero counts, maxes, and scatterplot of y.pred vs. y.
par(mfrow=c(1,3))
plot(table(n.zero.pred)/sum(table(n.zero.pred)),
     lwd = 3, xlim = c(15, 55), xaxt = 'n',
     main = 'Predicted Zero Counts',
     xlab = 'Number of Zeros', ylab = '')
  abline(v = sum(y==0), lty = 'dashed', lwd = 2, col = 'red')
  axis(1, seq(15, 55, 5))

plot(table(max.pred), xlim = c(6, 22), lwd = 3, xaxt = 'n',
     main = 'Predicted Maxima',
     xlab = 'Maximum', ylab = '')
  abline(v = max(y), lty = 'dashed', lwd = 2, col = 'red')
  axis(1, 6:22)

## It takes a bloody long time for this plot to load.
## Maybe take a random sample of, say, 100 ypreds?
set.seed(8234)
plot(x=y, y=y, xlim = c(0, max(y.pred) + 1), ylim=c(0, 12),
     type="n", xlab = expression(y[pred]), ylab = "y")
for(i in sample(nrow(y.pred), 100, replace = FALSE)){
  points(x=y.pred[i,], y=y, pch=21, col=rgb(0.3, 0.3, 1),
         bg = rgb(0, 1, 0, 0.004))
}
abline(a=0, b=1, col=rgb(1, 0, 0), lwd = 3, lty = "dashed")
par(mfrow=c(1,1))
@
\caption{TODO}
\label{post_pred}
\end{figure}

In Figure~\ref{post_pred} (left), the vertical dashed line represents the number of zeros in the original data.  The observed zero count is plausible under the distribution of posterior predictive zero counts, suggesting that the model has adequately captured this characteristic of the data.  Likewise, in figure 7 (center) it is apparent that the model was able to reproduce the maximum value of infected leaf counts in the data.  The scatterplot in Figure~\ref{post_pred} (right) shows a tendency for this model to slightly overpredict infected leaf counts, but examining the shading of the dots shows that an overwhelming majority of posterior predictive counts resemble the original data.

Posterior predictive checks need not only be used to make graphical comparisons.  Meaningful aspects of the data can be compared to values predicted by the model using numerical summaries.  In this study, plots with infection counts greater than three are of practical interest since it would represent an infection rate of more than 10\%. In the posterior predictions generated, the probability of an infected leaf count greater than three for a plot is $\Sexpr{yp.g3}$, which is fairly close to the observed proportion of $\Sexpr{y.g3}$.  A calculation of absolute error produces an average infected leaf count difference of $\Sexpr{m.abs.err.pred}$ with a 95\% posterior predictive interval of $(\Sexpr{abs.err.int[1]},\Sexpr{abs.err.int[2]})$.


\section{Bayesian ANOVA}

\begin{figure}[H]
<<bayes_anova,out.height='0.5\\linewidth',fig.height=4,fig.width=8>>=
# Gelman ANOVA plot for superpop sds
batches <- c('block',
              NA,
              'variety',
              'block * variety',
              NA,
              'inoculation',
              'nitrogen',
              NA,
              'block * inoculation',
              'block * nitrogen',
              'inoculation * nitrogen',
              'inoculation * variety',
              'nitrogen * variety',
              NA,
              'block * inoculation * nitrogen',
              'block * inoculation * variety',
              'block * nitrogen * variety',
              'inoculation * nitrogen * variety')
superpop.sds <- c("sigma_b", NA,
                   "sigma_v",
                   "sigma_bv", NA,
                   "sigma_i", "sigma_n", NA,
                   "sigma_bi", "sigma_bn", "sigma_ni",
                   "sigma_vi", "sigma_vn", NA,
                   "sigma_bni", "sigma_bvi",
                   "sigma_bvn", "sigma_vni")
dfs <- c(n.b, NA, n.v, n.bv, NA, n.i, n.n, NA, n.bi, n.bn, n.ni,
          n.vi, n.vn, NA, n.bni, n.bvi, n.bvn, n.vni) - 1
anova.superpop <- function(stan.fit, sources, sds, dfs, xlim = c(0, 1),
                           inner = 0.5, outer = 0.95,
                           width = c(7, 1, 22)){
# Creates a caterpillar plot comparing posterior intervals for each of
# several parameters for several values of one hyperparameter.
#  stan.fit  A stan_fit objects
#  sources   Character vector of batches being analyzed
#  sds       Character vector of the scale parameters being analyzed
#  dfs       Numeric vector of degrees of freedom for each batch
#  inner     Confidence lever for outer interval
#  outer     Confidence level for outer interval
#  width     Numeric vector of widths to send to layout()
  layout(matrix(1:3, nrow = 1), width = width)
  # Sources
  par(mar = c(4.1, 1.1, 5.1, 1.1))
  plot(NULL, xlim = c(0, 1), ylim = c(length(sources), 1), main = 'Source',
       xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', frame.plot = FALSE)
  text(x = 1, y = 1:length(sources), labels = sources, pos = 2, offset = 0)
  # dfs
  par(mar = c(4.1, 0, 5.1, 0))
  plot(NULL, xlim = c(0, 1), ylim = c(length(sources), 1), main = 'df',
       xlab = '', ylab = '', xaxt = 'n', yaxt = 'n', frame.plot = FALSE)
  text(x = 1, y = 1:length(sources), labels = dfs, pos = 2, offset = 0)
  # SDs
  par(mar = c(4.1, 1.1, 5.1, 1.1))
  plot(NULL, xlim = xlim, ylim = c(length(sds), 1),
       main = 'Posterior sd of Effects', xlab = '', ylab = '', yaxt = 'n')
  axis(3)
  for(j in 1:length(sources)){
    if(!is.na(sources[j])){
      current <- unlist(extract(stan.fit, pars = sds[j]))
      segments(x0 = quantile(current, c((1-outer)/2, (1-inner)/2)),
               x1 = quantile(current, 1-c((1-outer)/2, (1-inner)/2)),
               y0 = j, lwd = c(1, 3))
      points(x = median(current), y = j, pch = '|')
    }
  }
}

anova.superpop(od.stan, batches, superpop.sds, dfs, xlim = c(0, 10))
@
\caption{TODO}
\label{anova}
\end{figure}

\section{Model Comparison}
-code complexity
--not entirely sure to what degree glmer can actually fit, although it is simpler to use
--Bayesian modeling requires much more setup, but after that's done you can do more with it

-inferences that can be made from each model
--glmer gives point estimates, CIs and a single variance for the random effects
--with Bayes we have distributions for everything.  this provides much more information.

-model complexity
--easier to implement difficult model structures in Bayes
--Bayes can handle separation issues much better

-actual comparison of output
--variances(block)
--glmer has significance stars (!)
--

\section{Conclusion}



\end{document}