\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% packages
\usepackage{graphicx} % for pictures
%\usepackage{fancyhdr} % for headers
\usepackage{verbatim} % displaying r code
%\usepackage{fancyvrb}
\usepackage{setspace} % vspace and hspace
%\usepackage{listings}
\usepackage{enumitem} % for [label=]
\usepackage{amsmath}  % math symbols and such
\usepackage{amssymb}
%\usepackage{lastpage} % for page numerbering
\usepackage{color}
\usepackage{multirow}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% margins
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}

\setlength{\parskip}{5.5pt} % Skip half a line before and after each par
\setlength{\parindent}{0pt} % No auto-indent
% \def\fs{\footnotesize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%put%words%in%circle%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage{tikz}
% \usetikzlibrary{shapes.misc,shadows}
% \usetikzlibrary{arrows}
% \usetikzlibrary{shapes}
% \newcommand{\mymk}[1]{%
%   \tikz[baseline=(char.base)]\node[anchor=south west,
%     draw = blue, rectangle, rounded corners, inner sep=2pt,
%     minimum size=7mm, text height=2mm](char){\ensuremath{#1}} ;}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% title page
\title{Team BadAss}
\author{K. Flagg, B. Fenton, \& D. Anderson}
\date{}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle
\tableofcontents
% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% R code options
<<setup, include=FALSE, cache=FALSE>>=
### set default values for code chunks
opts_chunk$set(
  echo=FALSE,
  message=F,
  warning=F,
  fig.width=9,
  fig.height=6,
  out.width='\\linewidth',
  dev='pdf',
  concordance=TRUE,
  results='asis',
  size = 'footnotesize',
  fig.align='center'
)

### other defaults
options(
  replace.assign=TRUE,
  width=72,
  digits = 3,
  max.print="72",
  show.signif.stars = FALSE
)

### load in necessary packages
# require(nlme)
require(lme4)
require(xtable)
# require(ggplot2)
# require(lattice)
# require(rjags)
# require(R2jags)
# require(mcmcplots)
# require(LearnBayes)
# library(mvtnorm)
# library(coda)
# require(geoR) # for scaled inverse chi squared
require(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
### set working directory - Doug
# setwd("C:/Users/dsand_000/Desktop/Stats/S532/gitProj/Bayes-Group/report")
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
Bayesian inference is becoming a popular topic as of late; such is the case for fields like Ecology. However, the practice of Bayesian data analyis is rarely utilized in the Consulting Seminar for clients because few are familiar with the methodology. We hope to illustrate that, while Bayesian data analysis may be less familiar than its likelihood or frequentist counterparts, it can provide results when other approaches do not. Additionally, it is our goal to provide easy to understand implementations of Bayesian inference. This report will implement both a likelihoodist and Bayesian approach in order to compare results.

\section{Objectives}
Researchers are interested in the effect of the timing of a nitrogen treatment on the rate of Wheat Streak Mosaic Virus infection for several varieties of winter wheat. It is also of interest to see whether the strength of the effect differs between different varieties, or between environments where the virus is present in high levels versus low level. In this report, we will demonstrate the Analysis of Variance to determine which variables and interaction are the most important in explaining variation in infection rate.

\section{Design}
The experiment was performed in six blocks. Each block contained five rows, with one of five varieties of wheat randomly assigned to each row. Each row was split into six plots, and each plot was assigned a unique combination of nitrogen application time (early, mid, and late) and inoculation status (inoculated or not). Infected plants were transplanted into the inoculated plots so that the inoculated plots were known to have an elevated virus presence. Non-inoculated plots were assumed to have the virus present in a naturally-occurring level.

In total, 180 plots were observed. From each plot, 30 leaves were collected haphazardly and the infection status was recorded for each. These data were aggregated to the plot level so that the response variable was the number of infected plants out of 30.

\section{Simulation}
<<data_simulation>>=
## These functions will be used thoughout.
logit <- function(x){return(log(x/(1-x)))}
expit <- function(x){return(exp(x)/(1+exp(x)))}

## Here, we set up a data frame to store our data: factors of each
##   variable for each plot.
design <- data.frame('block' = as.numeric(gl(6, 30)),
                     'row' = as.numeric(gl(30, 6)),
                     'plot' = 1:180,
                     'variety' = numeric(180),
                     'inoc' = numeric(180),
                     'nitrogen' = numeric(180))
set.seed(6823)
## Each column is a permutation of variety indices
permute.v <- matrix(replicate(6, sample(5, 5, replace = FALSE)), ncol = 6)
for(i in 1:180){
  design$variety[i] <- permute.v[(design$row[i]-1)%%5+1, design$block[i]]
}
ni <- matrix(c(1, 1, 1, 2, 2, 2, 1:3, 1:3), ncol = 2)
## Each column is a permutation of inoc:nitrogen indices
permute.ni <- matrix(replicate(30, sample(6, 6, replace = FALSE)), ncol = 30)
for(i in 1:30){
  design[((i-1)*6+1):(i*6), c('inoc', 'nitrogen')] <- ni[permute.ni[,i],]
}



#### Start the process of simulating data.
## For the binomial setting we have
##    y ~ Bin(30,pi) where
##    logit(pi) = Xb and
##    X is the model matrix and b is a vector of coefficients

# Determine a value of mu to be used for the intercept (first row of b vector).
#   We found that logit(0.05) is approximately -3.
mu <- -3

# It is good practice to set seed when simulating data as it will be
#   reproducible.
set.seed(93)

## We can set values for the main coefficients or use random draws.
##   These are also rows of the b vector.
# There are six blocks.
b.eff <- rnorm(6,0,0.01)
# There are five varieties.
v.eff <- rnorm(5,0,0.3)
# There are three nitrogen levels.
n.eff <- sort(rnorm(3,0,0.4))
# There are two inoculation levels.
i.eff <- c(-0.5,0.5)

## We can do the same for all the two-way interactions.
##   These are more rows of the b vector.
# There are 30 combinations of block and variety.
bv.eff <- rnorm(30,0,0.00001)
# There are 18 combinations of block and nitrogen.
bn.eff <- rnorm(18,0,0.00001)
# There are 12 combinations of block and inoculation.
bi.eff <- rnorm(12,0,0.00001)
# There are 15 combinations of variety and nitrogen.
vn.eff <- rnorm(15,0,0.01)
# There are 10 combinations of variety and inoculation.
vi.eff <- rnorm(10,0,0.03)
# There are 6 combinations of nitrogen and inoculation.
ni.eff <- rnorm(6,0,0.03)

## We can also establish effects for three-way interactions.
##   These are also in the b vector.
# There are 90 combinations of block, variety, and nitrogen.
bvn.eff <- rnorm(90,0,0.00001)
# There are 60 combinations of block, variety, and inoculation.
bvi.eff <- rnorm(60,0,0.00001)
# There are 36 combinations of block, nitrogen, and inoculation.
bni.eff <- rnorm(36,0,0.00001)
# There are 30 combinations of variety, nitrogen, and inoculation.
vni.eff <- rnorm(30,0,0.001)

## It was not of interest to include a four-way interaction.



#### We can now update our "design" matrix with the levels of each
####   interaction.
# head(design) # reminder of the layout of design
## Find the levels for each two-way interaction assigned to a plot.
bv.int <- as.numeric(interaction(design[,1],design[,4]))
bn.int <- as.numeric(interaction(design[,1],design[,6]))
bi.int <- as.numeric(interaction(design[,1],design[,5]))
vn.int <- as.numeric(interaction(design[,4],design[,6]))
vi.int <- as.numeric(interaction(design[,4],design[,5]))
ni.int <- as.numeric(interaction(design[,6],design[,5]))
## Find the levels for each three-way interaction assigned to a plot.
bvn.int <- as.numeric(interaction(design[,1],design[,4],design[,6]))
bvi.int <- as.numeric(interaction(design[,4],design[,6]))
bni.int <- as.numeric(interaction(design[,4],design[,5]))
vni.int <- as.numeric(interaction(design[,6],design[,5]))

## Once the levels have been established for each interaction (of
##   each plot) we can include them as columns in our data set.
##   We have opted to call the data set 'od' for original data.
od <- data.frame(
  "blk" = factor(design[,1]),
  "vty" = factor(design[,4]),
  "nit" = factor(design[,6]),
  "ino" = factor(design[,5]),
  "bv.int" = factor(bv.int),
  "bn.int" = factor(bn.int),
  "bi.int" = factor(bi.int),
  "vn.int" = factor(vn.int),
  "vi.int" = factor(vi.int),
  "ni.int" = factor(ni.int),
  "bvn.int" = factor(bvn.int),
  "bvi.int" = factor(bvi.int),
  "bni.int" = factor(bni.int),
  "vni.int" = factor(vni.int)
)

## Using the levels of each factor for a plot, we can find the probabilities
##   associated with the simulated data.
# We can start on the logit scale: logit(pi) = Xb.
logit.pi.vec <- mu + b.eff[od[,1]] + v.eff[od[,2]] + n.eff[od[,3]] +
  i.eff[od[,4]] + bv.eff[od[,5]] + bn.eff[od[,6]] + bi.eff[od[,7]] +
  vn.eff[od[,8]] + vi.eff[od[,9]] + ni.eff[od[,10]] + bvn.eff[od[,11]] +
  bvi.eff[od[,12]] + bni.eff[od[,13]] + vni.eff[od[,14]]

# Now, transform those back to the pi (probability) scale.
pi.vec <- apply(cbind(logit.pi.vec), 1, expit) # pi = expit(X * beta)

# We can use the probabilities to generate y's: y_i ~ Bin(30,pi_i). These will
#   be the counts of infected leaves for a given plot.
set.seed(324)
y.vec <- apply(cbind(pi.vec), 1, function(p) rbinom(1,30,p))

## Lastly, we can attach the counts of infected leaves and uninfected leaves
##   for each plot to our data set.
od$"infected" <- y.vec
od$"uninfected" <- 30 - y.vec

## If desired, it is possible to write the data set to a csv or table for
##   future use.
# write.csv(od,"od.csv",row.names = FALSE)
@

Data were simulated from the model
\begin{align*}
y_j &\sim \text{Bin}(30,\pi_j), \ \text{for} \ j=1,2,\dots,180; \\
\text{logit}(\pi_j)
% OVERALL
&= \mu % Batch 0: mean
% BLOCK LEVEL
+ \beta^{(1)}_{b[j]} % Batch 1: Blk
% ROW LEVEL
+ \beta^{(2)}_{v[j]} % Batch 2: Vty
+ \beta^{(3)}_{b[j],v[j]} % Batch 3: Blk * Vty
% PLOT LEVEL
+ \beta^{(4)}_{i[j]} % Batch 4: Ino
+ \beta^{(5)}_{n[j]} % Batch 5: Nit
\\
&\quad
+ \beta^{(6)}_{b[j],i[j]} % Batch 6: Blk * Ino
+ \beta^{(7)}_{b[j],n[j]} % Batch 7: Blk * Nit
+ \beta^{(8)}_{i[j],n[j]} % Batch 8: Ino * Nit
+ \beta^{(9)}_{i[j],v[j]} % Batch 9: Ino * Vty
+ \beta^{(10)}_{n[j],v[j]} % Batch 10: Nit * Vty
\\
&\quad
+ \beta^{(11)}_{b[j],i[j],n[j]} % Batch 11: Blk * Ino * Nit
+ \beta^{(12)}_{b[j],i[j],v[j]} % Batch 12: Blk * Ino * Vty
+ \beta^{(13)}_{b[j],n[j],v[j]} % Batch 13: Blk * Nit * Vty
+ \beta^{(14)}_{i[j],n[j],v[j]}, % Batch 14: Ino * Nit * Vty
% Maybe add the four-way?
\end{align*}
where \(b=1,2,\dots,6\) indexes
the blocks; \(i=1\) indicates non-inoculated status and \(i=2\) indicates
inoculated status; \(n=1,2,3\) indicate early, mid, and late nitrogen
application, respectively; \(v=1,2,\dots,5\) index.

Given the study design we'd expect roughly 5\% of the leaves collected to be infected.
With this in mind, an intercept value of -3 was chosen since $\text{logit}(0.05)\approx-3$.
For each batch of coefficients, values were generated via random draws from normal distributions
centered at zero. Variances were chosen for each effect to reflect our beliefs about the importance
of that effect on the response.  We have provided code so that all details and steps of the process
are available and reproducible.  The simulated leaf counts can be seen in Figure 1.

\begin{figure}[h!]
<<simulation_plot, out.width="0.6\\linewidth">>=
par(mfrow=c(1,1), mar=c(2,2,2,1))
plot(table(y.vec), ylab = "", xlab = "")
  points(sort(unique(y.vec)), table(y.vec), pch=19, col="blue")
@
\caption{The number of the 180 simulated plots that had 0 leaves, 1 leaf, $\hdots$, 11 leaves infected out of the 30 collected from each plot.}
\end{figure}

\newpage

\section{Generalized Linear (Mixed) Model}
\subsection{Likelihood}
The likelihood function provides a relative measure of how well a possible
value of the parameter \(\theta\) fits with the observed data \(y\), and
so it can be used to identify the value of \(\theta\) that is then used
as the estimate most consistent with \(y\). The likelihood function takes
its form from the probability model for \(y\), denoted \(p(y|\theta)\).
However, we discuss likelihoods after the data have been collected so we
consider \(y\) to be fixed and we examine how \(p(y|\theta)\) varies as a
function of \(\theta\).

Since \(p(y|\theta)\) is the probability of observing \(y\) for a given
\(\theta\), a \(\theta\) where \(p(y|\theta)\) is large is more
consistent with the obeserved data than a \(\theta\) value that makes
\(p(y|\theta)\) small. Then a reasonable approach to estimating
\(\theta\) is to find the value of \(\theta\) that maximizes
\(p(y|\theta)\) for the observed \(y\).


Note that the likelihood function depends on the particular data
that were observed. The data \(y'\) could very well have been collected
instead of \(y\), and then the likelihood function would be
\(p(y'|\theta)\), which might be maximized by a different \(\theta\) value
than the \(\theta\) value the maximizes \(p(y|\theta)\). If we consider
how this estimate of \(\theta\) will vary between different samples,
we can construct a probability distribution.


\subsection{Likelihood Principle}
The idea behind the likelihoodist approach to making inference stems from the Likelihood Principle. A likelihoodist believes that given a statistical (probability) model, all of the information in a sample relevant to model parameters is contained in the likelihood function. The benefit to using the maximum likelihood estimate (MLE) is that, assuming the model is correct, the sample size is large enough and the sampling method is unbiased, then  the MLEs are unbiased, the standard deviations of the sampling distributions of the estimators are easy to formulate, the estimators have relatively high precision, and the shapes of the sampling distribution are approximately normal (The Statistical Sleuth, page 611).

\subsection{Likelihood to GLMER}
The likelihoodist approach to making inferences about split-plot experiments requires the use of linear mixed models.
In this case we are analyzing simulated count data, so a generalized linear mixed model is appropriate.
The \texttt{glmer()} function from the \texttt{lme4} package in the base R distribution was used to fit this model.

As nitrogen and inoculation were pre-assigned to plots, they are treated as constants (in other words, fixed effects).  Block and variety levels can be considered as members of a larger population, and as a result will be treated as a varying, or random, effect.
<<glmm, eval=FALSE>>=
## The lme4 package has the glmer function.
require(lme4)

## Fit the model using glm.
glm.mod <- glmer(cbind(infected,uninfected) ~ (nit+ino)^2 + (1|blk/vty),
                 data=od, family=binomial(link = "logit"),
                 control=glmerControl(optimizer="bobyqa"))

## Produce summary output for the model.
# summary(glm.mod)
# anova(glm.mod, test="Chisq")
@

\subsection{Results}
-caterpillar plot?
-
\subsection{Diagnostics}
-binned residual plot
\subsection{Arguments against Likelihood}
While the likelihoodist approach can be powerful and elegant in its simplicity, it has some major shortcomings.  Even if the model is correct, two different models can have the same likelihood function (as is the case for binomial and negative binomial distributions).  Similarly, even if the sample is sufficiently large and was collected in such a way that it should be representative, it is still possible to end up with a sample that is not representative of the population.  Either of these flaws could be tempered by including information about prior beliefs or domain-specific knowledge relevant to the model, but the likelihoodist approach does not provide a way to do so.  In some this may not matter, but in many others the relevant information which is left unused by the likelihoodist approach can greatly improve both the ability to make inferences and the quality of those inferences.

\section{Bayesian Analysis}
\subsection{Background}
Probability theory provides the basic property of conditional probability known as Bayes rule,
\begin{align*}
p(\theta|y) & = \frac{p(\theta,y)}{p(y)} \\
& = \frac{p(\theta)p(y|\theta)}{p(y)}
\end{align*}
where $p(y) = \sum_{\theta} p(\theta)p(y|\theta)$ is summed over all possible values of $\theta$. A Bayesian analysis starts from Bayes rule. It involves finding a posterior distribution for
\(\theta\), denoted \(p(\theta|y)\), from the likelihood \(p(y|\theta)\)
and a prior distribution of \(\theta\), \(p(\theta)\). The posterior is
found using the proportionality relationship,
\begin{align*}
p(\theta|y)\propto p(\theta)\times p(y|\theta)\text{.}
\end{align*}
Often, analysts assume that all values of \(\theta\) are approximately
equally probable, and so the prior distribution is constant:
\(p(\theta)=c\). Then the posterior distribution is proportional to the
likelihood,
\begin{align*}
p(\theta|y)\propto c\times p(y|\theta)\propto p(y|\theta)\text{.}
\end{align*}
In cases like this, the value of \(\theta\) that is most probable in
posterior is the same as the maximum likelihood estimate, so it is common
for Bayesian inference to appear similar to likelihood-based inference.
It is important to remember, however, that a likelihood is not a probabilty
statement about $\theta$. A Bayesian posterior distribution is used to make
probability statements about \(\theta\), and a likelihood-based
\emph{estimator} has a probability distribution, but the likelihood itself
can only be used to compare possible \(\theta\) values in the context of
one set of observed data.

\subsection{Why Choose Bayesian}
There are many advantages to performing a Bayesian analysis. A big one in this case, is its ability to provide meaningful results. MLE at edge of parameter space.

The likelihoodist approach provides a point estimate; a Bayesian approach provides a probability distribution.

It is very difficult to fit a generalized linear mixed model in R, and can have separation issues. We can avoid many of those complications with the Bayesian approach.

It can also come in handy when not much data is provided. {\it This might not be necessary to include.}

\subsection{Priors}
As described in Section 6.1, the use of Bayesian analysis involves prior distributions. In practice, not much is known as to what distribution the parameters, themselves, actually follow. But, hey, we don't actually need to know! We ultimately want the data to make the most impact on the posterior distribution we obtain, so our decisions on what priors we use are really just to get the ball rolling, so to speak.

A natural choice for many who implement a Bayesian analysis is to use what is called a noninformative prior. It is noninformative in the sense that is has the largest possible variance. This variance allows for the parameter to be from anywhere the distribution is defined. Another choice is to use a minimally informative prior. There are typically some natural limits as to what that parameter could be.

There are some benefits to the choices of priors, such as conjugacy (but that doesn't come into play here as much).

\subsection{The Bayesian Model}
The model we have gone with is yada, yada, yada.

{\it Discuss the priors we chose here or create a subsection?}

The Normal probability distributions for each effect is a natural choice. Each centered at 0 as they will be adjustments on the overall mean. The half cauchy is a solid choice as argued by Gelman et al in a ``Weakly Informative Paper''. The spread of 3 allows for the effects to differ by an effect size of 18, which on the logit scale is enormous, as argued by some page in BDA3.

\subsection{McSTANislaw McUlam}
When conjugate priors are used we have the benefit of knowing the  distribution of the posterior. However, given the large capacity of parameters in the model we have fit and the prior distributions chosen, the posterior is not as straightforward. This would result in very intensive, and possibly lack of computing power, calculations to determine $p(y)$ -- the denominator in Section 6.1. However, there is a very convenient way around this issue.

Markov Chain Monte Carlo (MCMC) is the process of jumping around the parameter space in order to obtain draws from some target distribution (in this case the posterior distribution). As with all fields of mathematics and statistics we put our trust in the infinite; in that, it is the belief that if we were to sample enough times we will have converged on to a distribution that is the true posterior.

R isn't the fastest of programming environments, and given the quantity of parameters we hope to estimate, it would run slowly. The way we can harness the power of MCMC will be through STAN, which adapts the code to run in C -- much faster. There are alternative out there, but this one is rather brilliant. STAN uses a type of MCMC algorithm called Hamiltonian something or another which jumps around the parameter space very efficiently, like a marble rolling up, down, and around a bowl.

\subsection{Written model to STAN models}
We can take the model written above and transform it into computing lingo for STAN.

\subsection{Other compilers?}
There is also BUGS, JAGS, and BayesGlm. There is a website to help you choose priors, maybe?

\section{Posterior Inferences}


\section{Posterior Predictive Check}
\subsection{Overview}


\subsection{Test Quantities}
There are two particular features of the original data which are important to capture in the modeling process: the relatively large number of zeros, and the maximum infected count.  A model incorporating either of these features separately would be relatively straightforward to fit, but capturing both simultaneously takes more consideration.  Since the extreme values of the data are of primary concern here, the mean is not a useful metric.  In order to ascertain whether or not the model did in fact capture these aspects of the data, posterior predictive checks for the number of predicted zeros and predicted maximum were performed.  The relationship between predicted counts and the original counts can be visualized using a scatterplot.

Posterior predictive checks need not only be graphical.  Meaningful aspects of the data can be compared to values predicted by the model using numerical summaries.  In this study, plots with infection counts greater than 3 are of practical interest since it would represent an infection rate of more than 10\%.

\subsection{Implementation}


<<posterior_prediction_setup>>=
# Get the one Stan fit where we used a scale of 3
od.stan <- sens[[3]]

# Do some posterior predictin'
p.stan <- extract(od.stan, pars = 'p')$p
y.pred <- apply(p.stan, c(1, 2), function(p){return(rbinom(1, m, p))})

n.zero.pred <- apply(y.pred, 1, function(x){return(sum(x==0))})
max.pred <- apply(y.pred, 1, max)

rms <- function(ypred) {return(sqrt(mean((ypred-y)^2)))}
rms.pred <- apply(y.pred, 1, rms)

@
<<post_pred_zeros>>=

plot(table(n.zero.pred)/sum(table(n.zero.pred)),
     lwd = 3, xlim = c(15, 55), xaxt = 'n',
     main = 'Predicted Zero Counts',
     xlab = 'Number of Zeros', ylab = '')
abline(v = sum(y==0), lty = 'dashed', lwd = 2, col = 'red')
axis(1, seq(15, 55, 5))
@
<<post_pred_max>>=

plot(table(max.pred), xlim = c(6, 22), lwd = 3, xaxt = 'n',
     main = 'Predicted Maxima',
     xlab = 'Maximum', ylab = '')
abline(v = max(y), lty = 'dashed', lwd = 2, col = 'red')
axis(1, 6:22)
@

<<post_pred_scatt>>=
plot(x=y, y=y, xlim = c(0, max(y.pred) + 1), ylim=c(0, max(y.pred) + 1), 
     type="n", xlab = expression(y[pred]), ylab = "y")
for(i in 1:nrow(y.pred))
{
  points(x=y.pred[i,], y=y, pch=21, col=rgb(0.3, 0.3, 1), bg = rgb(0, 1, 0, 0.004))

}
  abline(a=0, b=1, col=rgb(1, 0, 0), lwd = 3, lty = "dashed")
@




\section{Bayesian ANOVA}
\section{Model Comparison}

\section{Conclusion}



\end{document}