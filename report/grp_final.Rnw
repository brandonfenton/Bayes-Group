\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% packages
\usepackage{graphicx} % for pictures
%\usepackage{fancyhdr} % for headers
\usepackage{verbatim} % displaying r code
%\usepackage{fancyvrb}
\usepackage{setspace} % vspace and hspace
%\usepackage{listings}
\usepackage{enumitem} % for [label=]
\usepackage{amsmath}  % math symbols and such
\usepackage{amssymb}
%\usepackage{lastpage} % for page numerbering
\usepackage{color}
\usepackage{multirow}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% margins
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}

\setlength{\parskip}{5.5pt} % Skip half a line before and after each par
\setlength{\parindent}{0pt} % No auto-indent
% \def\fs{\footnotesize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%put%words%in%circle%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \usepackage{tikz}
% \usetikzlibrary{shapes.misc,shadows}
% \usetikzlibrary{arrows}
% \usetikzlibrary{shapes}
% \newcommand{\mymk}[1]{%
%   \tikz[baseline=(char.base)]\node[anchor=south west,
%     draw = blue, rectangle, rounded corners, inner sep=2pt,
%     minimum size=7mm, text height=2mm](char){\ensuremath{#1}} ;}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% title page
\title{Team BadAss}
\author{K. Flagg, B. Fenton, \& D. Anderson}
\date{}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle
\tableofcontents
% \newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% R code options
<<setup, include=FALSE, cache=FALSE>>=
### set default values for code chunks
opts_chunk$set(
  echo=FALSE,
  message=F,
  warning=F,
  fig.width=9,
  fig.height=6,
  out.width='\\linewidth',
  dev='pdf',
  concordance=TRUE,
  results='asis',
  size = 'footnotesize',
  fig.align='center'
)

### other defaults
options(
  replace.assign=TRUE,
  width=72,
  digits = 3,
  max.print="72",
  show.signif.stars = FALSE
)

### load in necessary packages
# require(nlme)
# require(lme4)
require(xtable)
# require(ggplot2)
# require(lattice)
# require(rjags)
# require(R2jags)
# require(mcmcplots)
# require(LearnBayes)
# library(mvtnorm)
# library(coda)
# require(geoR) # for scaled inverse chi squared
# require(rstan)

### set working directory - Doug
# setwd("C:/Users/dsand_000/Desktop/Stats/S532/gitProj/Bayes-Group/report")
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
Bayesian inference is becoming a popular topic as of late; such is the case for fields like Ecology. However, the practice of Bayesian data analyis is rarely utilized in the Consulting Seminar for clients because few are familiar with the methodology. We hope to illustrate that, while Bayesian data analysis may be less familiar than its likelihood or frequentist counterparts, it can provide results when other approaches do not. Additionally, it is our goal to provide easy to understand implementations of Bayesian inference. This report will implement both a likelihoodist and Bayesian approach in order to compare results.

\section{Objectives}
Nar is interested in the effect that variety, application time of nitrogen, and inoculation have on the probability of virus infection in Winter Wheat. Of primary interest is the effect of application time of nitrogen, as well as assessing evidence for all possible two and three way interactions. The simplest possible model, to describe the effects of the above variables, is desirable. Throughout this report, it is our desire to address the following questions:
\begin{itemize}
\item Is there evidence that the interaction between time of nitrogen application and variety of wheat differs between plots where plants were inoculated or not?

\item Is there evidence that the time of nitrogen application varies across varieties of wheat? Additionally, is there evidence that the time of nitrogen application differs between plots where plants were inoculated or not?

\item Is there evidence that the variety of wheat differs between plots where plants were inoculated or not?

\item Conditional on the results to the above questions, how do we appropriately describe the estimated effects for variety, nitrogen application time, and inoculation on the
probability of virus infection?
\end{itemize}

\section{Design}
  In total there were six blocks. In each block there were five rows, each of which contained one variety of wheat. In each row there were six plots, each containing a unique combination of nitrogen application time and inoculation status (inoculated or not). In total there were 180 plots.  From each plot, 30 leaves were collected haphazardly and their infection statuses were recorded.

\section{Simulation}
<<data_simulation>>=
## These functions will be used thoughout.
logit <- function(x){return(log(x/(1-x)))}
expit <- function(x){return(exp(x)/(1+exp(x)))}

## Here, we set up a data frame to store our data: factors of each
##   variable for each plot.
design <- data.frame('block' = as.numeric(gl(6, 30)),
                     'row' = as.numeric(gl(30, 6)),
                     'plot' = 1:180,
                     'variety' = numeric(180),
                     'inoc' = numeric(180),
                     'nitrogen' = numeric(180))
set.seed(6823)
## Each column is a permutation of variety indices
permute.v <- matrix(replicate(6, sample(5, 5, replace = FALSE)), ncol = 6)
for(i in 1:180){
  design$variety[i] <- permute.v[(design$row[i]-1)%%5+1, design$block[i]]
}
ni <- matrix(c(1, 1, 1, 2, 2, 2, 1:3, 1:3), ncol = 2)
## Each column is a permutation of inoc:nitrogen indices
permute.ni <- matrix(replicate(30, sample(6, 6, replace = FALSE)), ncol = 30)
for(i in 1:30){
  design[((i-1)*6+1):(i*6), c('inoc', 'nitrogen')] <- ni[permute.ni[,i],]
}



#### Start the process of simulating data.
## For the binomial setting we have
##    y ~ Bin(30,pi) where
##    logit(pi) = Xb and
##    X is the model matrix and b is a vector of coefficients

# Determine a value of mu to be used for the intercept (first row of b vector).
#   We found that logit(0.05) is approximately -3.
mu <- -3

# It is good practice to set seed when simulating data as it will be
#   reproducible.
set.seed(93)

## We can set values for the main coefficients or use random draws.
##   These are also rows of the b vector.
# There are six blocks.
b.eff <- rnorm(6,0,0.01)
# There are five varieties.
v.eff <- rnorm(5,0,0.3)
# There are three nitrogen levels.
n.eff <- sort(rnorm(3,0,0.4))
# There are two inoculation levels.
i.eff <- c(-0.5,0.5)

## We can do the same for all the two-way interactions.
##   These are more rows of the b vector.
# There are 30 combinations of block and variety.
bv.eff <- rnorm(30,0,0.00001)
# There are 18 combinations of block and nitrogen.
bn.eff <- rnorm(18,0,0.00001)
# There are 12 combinations of block and inoculation.
bi.eff <- rnorm(12,0,0.00001)
# There are 15 combinations of variety and nitrogen.
vn.eff <- rnorm(15,0,0.01)
# There are 10 combinations of variety and inoculation.
vi.eff <- rnorm(10,0,0.03)
# There are 6 combinations of nitrogen and inoculation.
ni.eff <- rnorm(6,0,0.03)

## We can also establish effects for three-way interactions.
##   These are also in the b vector.
# There are 90 combinations of block, variety, and nitrogen.
bvn.eff <- rnorm(90,0,0.00001)
# There are 60 combinations of block, variety, and inoculation.
bvi.eff <- rnorm(60,0,0.00001)
# There are 36 combinations of block, nitrogen, and inoculation.
bni.eff <- rnorm(36,0,0.00001)
# There are 30 combinations of variety, nitrogen, and inoculation.
vni.eff <- rnorm(30,0,0.001)

## It was not of interest to include a four-way interaction.



#### We can now update our "design" matrix with the levels of each
####   interaction.
# head(design) # reminder of the layout of design
## Find the levels for each two-way interaction assigned to a plot.
bv.int <- as.numeric(interaction(design[,1],design[,4]))
bn.int <- as.numeric(interaction(design[,1],design[,6]))
bi.int <- as.numeric(interaction(design[,1],design[,5]))
vn.int <- as.numeric(interaction(design[,4],design[,6]))
vi.int <- as.numeric(interaction(design[,4],design[,5]))
ni.int <- as.numeric(interaction(design[,6],design[,5]))
## Find the levels for each three-way interaction assigned to a plot.
bvn.int <- as.numeric(interaction(design[,1],design[,4],design[,6]))
bvi.int <- as.numeric(interaction(design[,4],design[,6]))
bni.int <- as.numeric(interaction(design[,4],design[,5]))
vni.int <- as.numeric(interaction(design[,6],design[,5]))

## Once the levels have been established for each interaction (of
##   each plot) we can include them as columns in our data set.
##   We have opted to call the data set 'od' for original data.
od <- data.frame(
  "blk" = factor(design[,1]),
  "vty" = factor(design[,4]),
  "nit" = factor(design[,6]),
  "ino" = factor(design[,5]),
  "bv.int" = factor(bv.int),
  "bn.int" = factor(bn.int),
  "bi.int" = factor(bi.int),
  "vn.int" = factor(vn.int),
  "vi.int" = factor(vi.int),
  "ni.int" = factor(ni.int),
  "bvn.int" = factor(bvn.int),
  "bvi.int" = factor(bvi.int),
  "bni.int" = factor(bni.int),
  "vni.int" = factor(vni.int)
)

## Using the levels of each factor for a plot, we can find the probabilities
##   associated with the simulated data.
# We can start on the logit scale: logit(pi) = Xb.
logit.pi.vec <- mu + b.eff[od[,1]] + v.eff[od[,2]] + n.eff[od[,3]] +
  i.eff[od[,4]] + bv.eff[od[,5]] + bn.eff[od[,6]] + bi.eff[od[,7]] +
  vn.eff[od[,8]] + vi.eff[od[,9]] + ni.eff[od[,10]] + bvn.eff[od[,11]] +
  bvi.eff[od[,12]] + bni.eff[od[,13]] + vni.eff[od[,14]]

# Now, transform those back to the pi (probability) scale.
pi.vec <- apply(cbind(logit.pi.vec), 1, expit) # pi = expit(X * beta)

# We can use the probabilities to generate y's: y_i ~ Bin(30,pi_i). These will
#   be the counts of infected leaves for a given plot.
set.seed(324)
y.vec <- apply(cbind(pi.vec), 1, function(p) rbinom(1,30,p))

## Lastly, we can attach the counts of infected leaves and uninfected leaves
##   for each plot to our data set.
od$"infected" <- y.vec
od$"uninfected" <- 30 - y.vec

## If desired, it is possible to write the data set to a csv or table for
##   future use.
# write.csv(od,"od.csv",row.names = FALSE)
@
For this simulation the following model was used:
\begin{align*}
&y_i \sim \text{Bin}(30,\pi_i) \\
&\text{logit}(\pi_i) = {\bf x}_{i}{\bf \beta} % Bold? TODO
\end{align*}
Given the study design we'd expect roughly 5\% of the leaves collected to be infected.
With this in mind, an intercept value of -3 was chosen since $\text{logit}(0.05)\approx-3$.
For each batch of coefficients, values were generated via random draws from normal distributions
centered at zero. Variances were chosen for each effect to reflect our beliefs about the importance
of that effect on the response.  We have provided code so that all details and steps of the process
are available and reproducible.  The simulated leaf counts can be seen in Figure 1.

\begin{figure}[h!]
<<simulation_plot, out.width="0.6\\linewidth">>=
par(mfrow=c(1,1), mar=c(2,2,2,1))
plot(table(y.vec), ylab = "", xlab = "")
  points(sort(unique(y.vec)), table(y.vec), pch=19, col="blue")
@
\caption{The number of the 180 simulated plots that had 0 leaves, 1 leaf, $\hdots$, 11 leaves infected out of the 30 collected from each plot.}
\end{figure}

\newpage

\section{Generalized Linear (Mixed) Model}
\subsection{Likelihood}
The likelihood function provides a relative measure of how well a possible
value of the parameter \(\theta\) fits with the observed data \(y\), and
so it can be used to identify the value of \(\theta\) that is then used
as the estimate most consistent with \(y\). The likelihood function takes
its form from the probability model for \(y\), denoted \(p(y|\theta)\).
However, we discuss likelihoods after the data have been collected so we
consider \(y\) to be fixed and we examine how \(p(y|\theta)\) varies as a
function of \(\theta\).

Since \(p(y|\theta)\) is the probability of observing \(y\) for a given
\(\theta\), a \(\theta\) where \(p(y|\theta)\) is large is more
consistent with the obeserved data than a \(\theta\) value that makes
\(p(y|\theta)\) small. Then a reasonable approach to estimating
\(\theta\) is to find the value of \(\theta\) that maximizes
\(p(y|\theta)\) for the observed \(y\).


Note that the likelihood function depends on the particular data
that were observed. The data \(y'\) could very well have been collected
instead of \(y\), and then the likelihood function would be
\(p(y'|\theta)\), which might be maximized by a different \(\theta\) value
than the \(\theta\) value the maximizes \(p(y|\theta)\). If we consider
how this estimate of \(\theta\) will vary between different samples,
we can construct a probability distribution.


\subsection{Likelihood Principle}
The idea behind the likelihoodist approach to making inference stems from the Likelihood Principle. A likelihoodist believes that given a statistical (probability) model, all of the information in a sample relevant to model parameters is contained in the likelihood function. The benefit to using the maximum likelihood estimate (MLE) is that, assuming the model is correct, the sample size is large enough and the sampling method is unbiased, then  the MLEs are unbiased, the standard deviations of the sampling distributions of the estimators are easy to formulate, the estimators have relatively high precision, and the shapes of the sampling distribution are approximately normal (The Statistical Sleuth, page 611).

\subsection{Likelihood to GLMER}
The likelihoodist approach to making inferences about split-plot experiments requires the use of linear mixed models.
In this case we are analyzing simulated count data, so a generalized linear mixed model is appropriate.
The \texttt{glmer()} function from the \texttt{lme4} package in the base R distribution was used to fit this model.

As nitrogen and inoculation were pre-assigned to plots, they are treated as constants (in other words, fixed effects).  Block and variety levels can be considered as members of a larger population, and as a result will be treated as a varying, or random, effect.
<<glmm, eval=FALSE>>=
## The lme4 package has the glmer function.
require(lme4)
  
## Fit the model using glm.
glm.mod <- glmer(cbind(infected,uninfected) ~ (nit+ino)^2 + (1|blk/vty),
                 data=od, family=binomial(link = "logit"),
                 control=glmerControl(optimizer="bobyqa"))

## Produce summary output for the model.
# summary(glm.mod)
# anova(glm.mod, test="Chisq")
@

\subsection{Results}
-caterpillar plot?
-
\subsection{Diagnostics}
-binned residual plot
\subsection{Arguments against Likelihood}
\begin{itemize}
While the likelihoodist approach can be powerful and elegant in its simplicity, it has some major shortcomings.
\item bin, nbin have same lhoods
\item mle sucks if the data aren't really representative
\end{itemize}

\section{Bayesian Analysis}
\subsection{Background}
A Bayesian analysis involves finding a posterior distribution for
\(\theta\), denoted \(p(\theta|y)\), from the likelihood \(p(y|\theta)\)
and a prior distribution of \(\theta\), \(p(\theta)\). The posterior is
found using the proportionality relationship,
\begin{align*}
p(\theta|y)\propto p(\theta)\times p(y|\theta)\text{.}
\end{align*}
Often, analysts assume that all values of \(\theta\) are approximately
equally probable, and so the prior distribution is constant:
\(p(\theta)=c\). Then the posterior distribution is proportional to the
likelihood,
\begin{align*}
p(\theta|y)\propto c\times p(y|\theta)\propto p(y|\theta)\text{.}
\end{align*}
In cases like this, the value of \(\theta\) that is most probable in
posterior is the same as the maximum likelihood estimate, so it is common
for Bayesian inference to appear similar to likelihood-based inference.
It is important to remember, however, that a likelihood is not a probabilty
statement about theta. A Bayesian posterior distribution is used to make
probability statements about \(\theta\), and a likelihood-based
\emph{estimator} has a probability distribution, but the likelihood itself
can only be used to compare possible \(\theta\) values in the context of
one set of observed data.

\subsection{Why Choose Bayesian}

\subsection{Priors}

\subsection{STAN}

\section{Posterior Inferences}


\section{Posterior Predictive Check}
-
Ideas:
\begin{itemize}
\item counting number of zeros \checkmark
\item maximum \checkmark
\item pearson residuals
\item RMS density/hist
\item $y_{pred}$ versus $y_{sim}$ scatterplot
\end{itemize}


\section{Bayesian ANOVA}
\section{Model Comparison}

\section{Conclusion}



\end{document}